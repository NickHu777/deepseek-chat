"""
AIèŠå¤©APIè·¯ç”± - ä¿®æ”¹ç‰ˆï¼ˆæŒ‰å‰ç«¯è§„èŒƒï¼‰
å®ç°ï¼šPOST /chat, POST /chat/generate, æµå¼æ¥å£
å®Œå…¨æŒ‰å‰ç«¯è¦æ±‚ï¼šæ— å‰ç¼€ï¼Œæ­£ç¡®æ ¼å¼è¿”å›
"""

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
import json

from app.api.dependencies import (
    get_db_session,
    get_chat_message_service,
    get_ai_service
)
from app.models import ChatHistory
from app.schemas import (
    ChatRequest,
    ChatGenerateRequest, ChatHistoryResponse
)
from app.services import ChatMessageService, AIService
from app.services.exceptions import ServiceException, DatabaseException

router = APIRouter(tags=["AIèŠå¤©"])


@router.post(
    "/chat-histories/{chat_history_id}/messages",
    summary="å‘é€æ¶ˆæ¯å¹¶è·å–AIå›å¤",
    description="åœ¨æŒ‡å®šçš„èŠå¤©å†å²ä¸­å‘é€æ¶ˆæ¯å¹¶è·å–AIå›å¤"
)
async def send_chat_message(
        chat_history_id: int,
        message: str,  # ç›´æ¥æ¥æ”¶æ¶ˆæ¯å­—ç¬¦ä¸²
        message_service: ChatMessageService = Depends(get_chat_message_service),
        ai_service: AIService = Depends(get_ai_service),
        db: Session = Depends(get_db_session)
):
    """
    åœ¨èŠå¤©å†å²ä¸­å‘é€æ¶ˆæ¯å¹¶è·å–AIå›å¤

    - **chat_history_id**: èŠå¤©å†å²IDï¼ˆè·¯å¾„å‚æ•°ï¼‰
    - **message**: ç”¨æˆ·æ¶ˆæ¯å†…å®¹

    å“åº”æ ¼å¼ï¼š
    {
      "success": true,
      "user_message": {...},
      "ai_reply": {...}
    }
    """
    try:
        import asyncio
        from app.services import ChatHistoryService
        from app.schemas import ChatRequest

        # æ„å»º ChatRequest å¯¹è±¡
        chat_request = ChatRequest(message=message, chatId=chat_history_id)

        # 1. å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼ˆä¿å­˜åˆ°æ•°æ®åº“ï¼‰- åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œé¿å…é˜»å¡
        user_message = await asyncio.to_thread(
            message_service.create_user_message,
            chat_history_id=chat_request.chatId,
            content=chat_request.message
        )

        # 2. è·å–å¯¹è¯ä¸Šä¸‹æ–‡
        context = await asyncio.to_thread(
            message_service.get_conversation_context,
            chat_request.chatId
        )

        # 3. ç”ŸæˆAIå›å¤ - AIè°ƒç”¨å¯èƒ½è€—æ—¶ï¼Œä½¿ç”¨çº¿ç¨‹æ± 
        ai_result = await asyncio.to_thread(
            ai_service.process_chat_with_context,
            user_message.model_dump(),
            context
        )
        ai_reply_content = ai_result["reply"]

        # 4. ä¿å­˜AIå›å¤åˆ°æ•°æ®åº“
        ai_message = await asyncio.to_thread(
            message_service.create_ai_message,
            chat_history_id=chat_request.chatId,
            content=ai_reply_content
        )

        # 5. æ›´æ–°èŠå¤©å†å²æ ‡é¢˜ï¼ˆå¦‚æœæ˜¯ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼‰
        if len(context) == 0:
            history_service = ChatHistoryService(db)
            await asyncio.to_thread(
                history_service.update_chat_history_title_from_messages,
                chat_request.chatId
            )

        # 6. æŒ‰å‰ç«¯æ ¼å¼è¿”å› - ç›´æ¥ä½¿ç”¨ ChatMessageResponse å¯¹è±¡çš„å­—æ®µ
        return {
            "success": True,
            "user_message": {
                "id": user_message.id,
                "content": user_message.content,
                "sender": user_message.sender,
                "time": user_message.time
            },
            "ai_reply": {
                "id": ai_message.id,
                "content": ai_message.content,
                "sender": ai_message.sender,
                "time": ai_message.time
            }
        }

    except ServiceException as e:
        raise HTTPException(
            status_code=400 if hasattr(e, 'code') else 500,
            detail={
                "success": False,
                "error":  str(e),
                "code": e.code if hasattr(e, 'code') else 500
            }
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "success": False,
                "error": f"å¤„ç†èŠå¤©æ¶ˆæ¯å¤±è´¥: {str(e)}",
                "code": 500
            }
        )


@router.post(
    "/completions",
    summary="è·å–AIå›å¤ï¼ˆç‹¬ç«‹æ¥å£ï¼‰",
    description="æ ¹æ®ç”¨æˆ·è¾“å…¥ç”ŸæˆAIå›å¤ï¼Œæ— ä¸Šä¸‹æ–‡"
)
async def generate_ai_reply(
        prompt: str,  # ç›´æ¥æ¥æ”¶ prompt å­—ç¬¦ä¸²
        ai_service: AIService = Depends(get_ai_service)
):
    """
    ç”ŸæˆAIå›å¤ï¼ˆç‹¬ç«‹æ¥å£ï¼‰

    - **prompt**: ç”¨æˆ·çš„è¾“å…¥å†…å®¹

    å“åº”æ ¼å¼ï¼š
    {
      "success": true,
      "reply": "AIç”Ÿæˆçš„å›å¤å†…å®¹"
    }
    """
    try:
        import asyncio
        from app.schemas import ChatGenerateRequest
        
        # æ„å»º ChatGenerateRequest å¯¹è±¡
        generate_request = ChatGenerateRequest(prompt=prompt)
        
        # AIè°ƒç”¨å¯èƒ½è€—æ—¶ï¼Œä½¿ç”¨çº¿ç¨‹æ± é¿å…é˜»å¡
        result = await asyncio.to_thread(
            ai_service.process_chat_generate_request,
            generate_request
        )

        # æŒ‰å‰ç«¯æ ¼å¼è¿”å›
        return {
            "success": True,
            "reply": result["reply"]
        }

    except ServiceException as e:
        raise HTTPException(
            status_code=400 if hasattr(e, 'code') else 500,
            detail={
                "success": False,
                "error": str(e),
                "code": e.code if hasattr(e, 'code') else 500
            }
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "success": False,
                "error": f"ç”ŸæˆAIå›å¤å¤±è´¥: {str(e)}",
                "code": 500
            }
        )


# ============== éœ€è¦ä¿®æ”¹æœåŠ¡å±‚çš„éƒ¨åˆ† ==============

# æ³¨æ„ï¼šéœ€è¦åœ¨ app/services/chat_history_service.py ä¸­æ·»åŠ ä»¥ä¸‹æ–¹æ³•ï¼š

def create_chat_history_with_welcome(self, title: str) -> ChatHistoryResponse:
    """
    åˆ›å»ºèŠå¤©å†å²å¹¶è‡ªåŠ¨æ·»åŠ AIæ¬¢è¿æ¶ˆæ¯ - æŒ‰å‰ç«¯ç¤ºä¾‹

    å‰ç«¯ç¤ºä¾‹ä¸­ï¼Œåˆ›å»ºæ–°å¯¹è¯åä¼šè¿”å›ä¸€æ¡AIæ¬¢è¿æ¶ˆæ¯
    """
    try:
        # 1. åˆ›å»ºèŠå¤©å†å²
        db_chat_history = ChatHistory(title=title)
        self.db.add(db_chat_history)
        self.db.commit()
        self.db.refresh(db_chat_history)

        # 2. æ·»åŠ AIæ¬¢è¿æ¶ˆæ¯ï¼ˆæŒ‰å‰ç«¯ç¤ºä¾‹ï¼‰
        from app.models import ChatMessage, SenderType
        welcome_message = ChatMessage(
            chat_history_id=db_chat_history.id,
            content="ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ",
            sender=SenderType.AI
        )
        self.db.add(welcome_message)
        self.db.commit()
        self.db.refresh(welcome_message)

        # 3. è¿”å›åŒ…å«æ¶ˆæ¯çš„å†å²
        return ChatHistoryResponse.from_db_model(
            db_chat_history,
            include_messages=True
        )

    except Exception as e:
        self.db.rollback()
        raise DatabaseException(f"åˆ›å»ºèŠå¤©å†å²å¤±è´¥: {str(e)}")


# ============== æµå¼å¯¹è¯æ¥å£ ==============

@router.post(
    "/chat-histories/{chat_history_id}/messages/stream",
    summary="å‘é€æ¶ˆæ¯å¹¶è·å–AIæµå¼å›å¤ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰",
    description="åœ¨æŒ‡å®šçš„èŠå¤©å†å²ä¸­å‘é€æ¶ˆæ¯å¹¶ä»¥æµå¼æ–¹å¼è·å–AIå›å¤"
)
async def send_chat_message_stream(
        chat_history_id: int,
        message: str,
        message_service: ChatMessageService = Depends(get_chat_message_service),
        ai_service: AIService = Depends(get_ai_service),
        db: Session = Depends(get_db_session)
):
    """
    æµå¼å¯¹è¯æ¥å£ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰ - AI é€å­—è¾“å‡º

    ä¸ /chat-histories/{id}/messages åŠŸèƒ½ç›¸åŒï¼Œä½†ä½¿ç”¨æµå¼è¾“å‡º
    
    è¿”å›æ ¼å¼ï¼ˆSSEï¼‰ï¼š
    data: {"type": "token", "content": "ä½ "}
    data: {"type": "token", "content": "å¥½"}
    data: {"type": "done", "message_id": 123}
    """
    async def event_generator():
        import asyncio
        try:
            from app.services import ChatHistoryService
            from app.schemas import ChatRequest

            # ğŸ”¥ ç«‹å³å‘é€å¼€å§‹äº‹ä»¶ï¼Œè®©å‰ç«¯çŸ¥é“è¯·æ±‚å·²æ”¶åˆ°
            yield f"data: {json.dumps({'type': 'start', 'message': 'AIæ­£åœ¨æ€è€ƒ...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0)  # ç¡®ä¿ç«‹å³å‘é€

            # 1. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ - ä½¿ç”¨çº¿ç¨‹æ± é¿å…é˜»å¡
            chat_request = ChatRequest(message=message, chatId=chat_history_id)
            user_message = await asyncio.to_thread(
                message_service.create_user_message,
                chat_history_id=chat_request.chatId,
                content=chat_request.message
            )

            # å‘é€ç”¨æˆ·æ¶ˆæ¯å·²ä¿å­˜çš„ç¡®è®¤
            yield f"data: {json.dumps({'type': 'user_message', 'message_id': user_message.id}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0)  # ç¡®ä¿ç«‹å³å‘é€

            # 2. è·å–å¯¹è¯ä¸Šä¸‹æ–‡ - ä½¿ç”¨çº¿ç¨‹æ± 
            context = await asyncio.to_thread(
                message_service.get_conversation_context,
                chat_request.chatId
            )

            # 3. æµå¼ç”Ÿæˆ AI å›å¤ - åœ¨çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥ç”Ÿæˆå™¨
            full_reply = ""
            
            # ğŸ”¥ ä½¿ç”¨ run_in_executor åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥ç”Ÿæˆå™¨
            import concurrent.futures
            loop = asyncio.get_event_loop()
            
            # åˆ›å»ºåŒæ­¥ç”Ÿæˆå™¨
            stream_gen = ai_service.generate_reply_stream(
                prompt=user_message.content,
                context=context
            )
            
            # åœ¨çº¿ç¨‹æ± ä¸­é€ä¸ªè·å– token
            executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
            while True:
                try:
                    # åœ¨çº¿ç¨‹æ± ä¸­è·å–ä¸‹ä¸€ä¸ªtoken
                    token = await loop.run_in_executor(executor, lambda: next(stream_gen, None))
                    if token is None:
                        break
                    full_reply += token
                    yield f"data: {json.dumps({'type': 'token', 'content': token}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0)  # ğŸ”¥ å…³é”®ï¼šç¡®ä¿æ¯ä¸ªtokenç«‹å³å‘é€
                except StopIteration:
                    break

            # 4. ä¿å­˜å®Œæ•´çš„ AI å›å¤ - ä½¿ç”¨çº¿ç¨‹æ± 
            ai_message = await asyncio.to_thread(
                message_service.create_ai_message,
                chat_history_id=chat_request.chatId,
                content=full_reply
            )

            # 5. æ›´æ–°æ ‡é¢˜ - ä½¿ç”¨çº¿ç¨‹æ± 
            if len(context) == 0:
                history_service = ChatHistoryService(db)
                await asyncio.to_thread(
                    history_service.update_chat_history_title_from_messages,
                    chat_request.chatId
                )

            # å‘é€å®Œæˆäº‹ä»¶
            yield f"data: {json.dumps({'type': 'done', 'message_id': ai_message.id}, ensure_ascii=False)}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache, no-transform",
            "X-Accel-Buffering": "no",
            "Connection": "keep-alive",
            "Content-Encoding": "none"
        }
    )


@router.post(
    "/completions/stream",
    summary="è·å–AIæµå¼å›å¤ï¼ˆç‹¬ç«‹æ¥å£ï¼‰",
    description="æ ¹æ®ç”¨æˆ·è¾“å…¥ç”ŸæˆAIæµå¼å›å¤ï¼Œæ— ä¸Šä¸‹æ–‡"
)
async def generate_ai_reply_stream(
        prompt: str,
        ai_service: AIService = Depends(get_ai_service)
):
    """
    æµå¼ç‹¬ç«‹å¯¹è¯æ¥å£ï¼ˆæ— ä¸Šä¸‹æ–‡ï¼‰ - AI é€å­—è¾“å‡º

    ä¸ /completions åŠŸèƒ½ç›¸åŒï¼Œä½†ä½¿ç”¨æµå¼è¾“å‡º
    ä¸ä¿å­˜åˆ°æ•°æ®åº“ï¼Œä¸å…³è”èŠå¤©å†å²
    
    è¿”å›æ ¼å¼ï¼ˆSSEï¼‰ï¼š
    data: {"type": "token", "content": "ä½ "}
    data: {"type": "token", "content": "å¥½"}
    data: {"type": "done"}
    """
    async def event_generator():
        import asyncio
        try:
            # ğŸ”¥ ç«‹å³å‘é€å¼€å§‹äº‹ä»¶
            yield f"data: {json.dumps({'type': 'start', 'message': 'AIæ­£åœ¨æ€è€ƒ...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0)  # ç¡®ä¿ç«‹å³å‘é€
            
            # æµå¼ç”Ÿæˆ AI å›å¤ï¼ˆæ— ä¸Šä¸‹æ–‡ï¼‰ - åœ¨çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥ç”Ÿæˆå™¨
            import concurrent.futures
            loop = asyncio.get_event_loop()
            
            # åˆ›å»ºåŒæ­¥ç”Ÿæˆå™¨
            stream_gen = ai_service.generate_reply_stream(
                prompt=prompt,
                context=None
            )
            
            # åœ¨çº¿ç¨‹æ± ä¸­é€ä¸ªè·å– token
            executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
            while True:
                try:
                    token = await loop.run_in_executor(executor, lambda: next(stream_gen, None))
                    if token is None:
                        break
                    yield f"data: {json.dumps({'type': 'token', 'content': token}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0)  # ğŸ”¥ å…³é”®ï¼šç¡®ä¿æ¯ä¸ªtokenç«‹å³å‘é€
                except StopIteration:
                    break

            # å‘é€å®Œæˆäº‹ä»¶
            yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache, no-transform",
            "X-Accel-Buffering": "no",
            "Connection": "keep-alive",
            "Content-Encoding": "none"
        }
    )
